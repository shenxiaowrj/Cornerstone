{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c87a7aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e253ef88",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.cuda' has no attribute 'is_avaiable'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_avaiable\u001b[49m()\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torch.cuda' has no attribute 'is_avaiable'"
     ]
    }
   ],
   "source": [
    "torch.cuda.is_avaiable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2657e7c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f59320bc",
   "metadata": {},
   "source": [
    "# 神经网络通常是由层或者模块来构成的，并且他们基于数据会进行操作。torch.nn提供了构建神经网络模型需要的所有模块。在Pytorch中每一个模块都是nn.module父类的继承也就是一个子类。一个神经网络本身就是一个模块，同时又包含了其他很多模块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5258910",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets,transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f011349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "#提前确定好在什么设备上训练模型\n",
    "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ecf1a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义分类模型网络模块\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork,self).__init__()\n",
    "        self.flatten=nn.Flatten()\n",
    "        self.linear_relu_stack=nn.Sequential(\n",
    "            nn.Linear(28*28,512),  #第0层 ##线性层即MLP前馈网络，第一个是输入特征的特征维度，\n",
    "            #第二个是隐含层的大小\n",
    "            nn.ReLU(),   #非线性激活函数\n",
    "            nn.Linear(512,512), #第2层\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,10),#第4层\n",
    "        )\n",
    "    def forward(self,x):  #前向运算\n",
    "        x=self.flatten(x)\n",
    "        logits=self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49cf6bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#用这个模块之前要实例化\n",
    "model=NeuralNetwork().to(device)\n",
    "print(model)\n",
    "#把model搬到了device中"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3a0338",
   "metadata": {},
   "source": [
    "用torchsummary中的summary:from torchsummary import summary\n",
    "把model放入summary中，随便传入一个形状，能看到每一层名称，形状，参数数目等等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd60cb5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class:tensor([3], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "#如果要使用model，直接传入数据即可。这一步调用了forward方法，但不需要直接调用\n",
    "X=torch.rand(1,28,28,device=device)  #在device上初始化\n",
    "logits=model(X) #作为model的forward函数的一个参数传入进来，最终得到logits，\n",
    "#作为softmanx层的输入\n",
    "pred_probab=nn.Softmax(dim=1)(logits) #dim确定对哪一个维度的softmax层进行归一化\n",
    "#得到的pred_probab是一个预测概率\n",
    "y_pred=pred_probab.argmax(1) #对得到的概率算出最大值，就能得到每个样本预测的分类值\n",
    "print(f\"Predicted class:{y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8e40a3",
   "metadata": {},
   "source": [
    "模型具体的层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7bbf2d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "input_image=torch.rand(3,28,28) #随机均匀生成一个3*28*28的张量\n",
    "print(input_image.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a350761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 784])\n"
     ]
    }
   ],
   "source": [
    "flatten=nn.Flatten()#nn.Flatten：从第一维到最后一维浓缩成一个维度，只保留batchsize\n",
    "#和所有其他维度相乘后的维度 即最后保留两个维度\n",
    "flat_image=flatten(input_image)\n",
    "print(flat_image.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00153717",
   "metadata": {},
   "source": [
    "28*28=784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1e00798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "layer1=nn.Linear(in_features=28*28,out_features=20) #先将线性层实例化\n",
    "hidden1=layer1(flat_image)\n",
    "print(hidden1.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac4df44c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLU:tensor([[ 1.0560e-01,  3.3969e-02,  3.7232e-01, -2.3890e-01,  3.5568e-01,\n",
      "         -2.8359e-01, -4.5223e-01,  3.3610e-01, -7.6581e-03, -3.6852e-01,\n",
      "          4.0717e-01, -2.8724e-02, -1.6174e-03,  1.8353e-01,  2.9763e-01,\n",
      "          5.9409e-02, -4.2121e-01, -6.1036e-02,  6.9084e-01, -3.5956e-01],\n",
      "        [ 3.2734e-01,  8.7424e-02,  3.3259e-01, -6.5141e-02,  5.1348e-01,\n",
      "         -2.9563e-01, -4.6552e-01,  5.3475e-01,  1.7270e-01, -5.6229e-01,\n",
      "          5.8176e-01, -2.3416e-01, -3.3694e-02, -2.0387e-01,  7.2927e-01,\n",
      "          5.1680e-03, -2.0514e-01, -1.8871e-01,  4.9582e-01, -3.0600e-01],\n",
      "        [ 1.1263e-01,  2.3463e-01,  7.2613e-01,  1.5204e-01,  5.1447e-01,\n",
      "         -1.0341e-01, -3.8089e-01,  6.4183e-01,  8.6425e-02, -5.2572e-01,\n",
      "          2.6542e-01, -3.6505e-02,  1.2920e-02, -2.1576e-01,  4.4612e-01,\n",
      "          5.2473e-04, -3.6033e-01,  2.1169e-01,  5.2817e-01, -2.0145e-01]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n",
      "After ReLU:tensor([[1.0560e-01, 3.3969e-02, 3.7232e-01, 0.0000e+00, 3.5568e-01, 0.0000e+00,\n",
      "         0.0000e+00, 3.3610e-01, 0.0000e+00, 0.0000e+00, 4.0717e-01, 0.0000e+00,\n",
      "         0.0000e+00, 1.8353e-01, 2.9763e-01, 5.9409e-02, 0.0000e+00, 0.0000e+00,\n",
      "         6.9084e-01, 0.0000e+00],\n",
      "        [3.2734e-01, 8.7424e-02, 3.3259e-01, 0.0000e+00, 5.1348e-01, 0.0000e+00,\n",
      "         0.0000e+00, 5.3475e-01, 1.7270e-01, 0.0000e+00, 5.8176e-01, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 7.2927e-01, 5.1680e-03, 0.0000e+00, 0.0000e+00,\n",
      "         4.9582e-01, 0.0000e+00],\n",
      "        [1.1263e-01, 2.3463e-01, 7.2613e-01, 1.5204e-01, 5.1447e-01, 0.0000e+00,\n",
      "         0.0000e+00, 6.4183e-01, 8.6425e-02, 0.0000e+00, 2.6542e-01, 0.0000e+00,\n",
      "         1.2920e-02, 0.0000e+00, 4.4612e-01, 5.2473e-04, 0.0000e+00, 2.1169e-01,\n",
      "         5.2817e-01, 0.0000e+00]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#nn.ReLU:非线性层，非线性激活函数，使神经网络建模能力更强大，通常是在线性变换之后加入到\n",
    "#网络中  ##类class必须实例化后才可以进行操作\n",
    "print(f\"Before ReLU:{hidden1}\\n\\n\")\n",
    "hidden1=nn.ReLU()(hidden1)#对ReLU实例化\n",
    "print(f\"After ReLU:{hidden1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3469f9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nn.Sequential:关于模块有序的容器 （下下次直播具体讲解）\n",
    "seq_modules=nn.Sequential(#数据会有序地经过以下模块，最终得到一些结果\n",
    "    flatten,\n",
    "    layer1,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 10)\n",
    ")\n",
    "input_image=torch.rand(3,28,28)\n",
    "logits=seq_modules(input_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90023759",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nn.softmax：指数后的归一化 \n",
    "softmax=nn.Softmax(dim=1)\n",
    "pred_probab=softmax(logits)#关于10个类的分布后的概率分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e58f8da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure: NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ") \n",
      "\n",
      "\n",
      "Layer:linear_relu_stack.0.weight|Size:torch.Size([512, 784])|Values:tensor([[-0.0249, -0.0281, -0.0265,  ..., -0.0224, -0.0049, -0.0328],\n",
      "        [-0.0141, -0.0014, -0.0018,  ...,  0.0141, -0.0343,  0.0018]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "Layer:linear_relu_stack.0.bias|Size:torch.Size([512])|Values:tensor([-0.0033, -0.0157], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "Layer:linear_relu_stack.2.weight|Size:torch.Size([512, 512])|Values:tensor([[ 0.0159, -0.0197,  0.0130,  ..., -0.0361,  0.0075, -0.0092],\n",
      "        [ 0.0119, -0.0125,  0.0022,  ...,  0.0394, -0.0074, -0.0122]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "Layer:linear_relu_stack.2.bias|Size:torch.Size([512])|Values:tensor([-0.0374,  0.0437], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "Layer:linear_relu_stack.4.weight|Size:torch.Size([10, 512])|Values:tensor([[-0.0101,  0.0336,  0.0134,  ...,  0.0120,  0.0303, -0.0036],\n",
      "        [-0.0249,  0.0179, -0.0006,  ...,  0.0409, -0.0359,  0.0133]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "Layer:linear_relu_stack.4.bias|Size:torch.Size([10])|Values:tensor([-0.0393, -0.0264], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#打印模型参数，例如值，大小等\n",
    "print(\"Model structure:\",model,\"\\n\\n\")\n",
    "for name,param in model.named_parameters():\n",
    "    print(f\"Layer:{name}|Size:{param.size()}|Values:{param[:2]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5457ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
