张量的运算API上：
displit和hsplit两种不同的分割
dstack沿着深度的那一维，将多个张量堆叠起来
gather沿着某一个维来去取一些变量，input，dim，index的参数必须传
reshape能够得到具有相同数据的张量和相同元素的张量，参数是input和shape
scatter_  将张量src中所有元素写入到当前张量中，通过index指明要写入哪些位置的索引
scatter_add_ 稀疏的加法，挑一些位置加元素
split 可以指定划分比例的chunks（chunk只能均分），split用的频率更高
squeeze 将所有维度为1的input移除掉，example:(A*B*1*C*1*D),squeeze后是(A*B*C*D)
stack 沿着某一个新的维度将一系列的张量拼接起来，所有的张量要求同样大小，参数是张量和dim（dim默认是0）

张量的运算API下：
torch.dytype 各种数据类型
函数swapaxes和swapdims跟transports差不多
在pytorch教学中有的API能定位到源码，如果这个API有python代码能定位
torch.take(input,index) input就是tensor,index就是longtensor，在index索引角度下会把input看成一维张
【重要】torch.tile(input,dims) 表示复制，如果传入的参数比tensor维度要少的话，前边的维度默认填充为1
torch.transpose(input,dim0,dim1) 进行转置，dim0和dim1就是即将交换的维度
torch.unbind(input,dim=0) 移除一个张量的维度
torch.unsqueeze(input,dim) 在某些特定的维度上新增一个维度，增维前后张量数目不变
【常用】torch.where(condition,x,y)  判断语句，根据condition判断返回x还是y，成立的话返回x否则是返回y
torch.manual_seed(seed) 为生成随机数设定一个种子，固定种子后，每次运行代码都会从同样的分布中随机采样随机数（有python源码）
一些随机函数：
torch.bernoulli(input) 返回的只有0或1
【常用】torch.normal 返回的是正态分布（高斯分布）
torch.rand 在[0,1)区间随机
torch.randint(low,high,size) 在区间取随机整数
torch.randn(size) 正态分布中采样得到随机数
torch.randperm(n) 对n个数随机组合，从0开始数