torch.utils.data.DataLoader 对应多个样本的，把样本变成随机算法

torch.utils.data.DataSet 用来处理单个训练样本的，如何从磁盘中读取训练数据集包括特征和标签等，做点函数预处理，把磁盘映射成xy
dataset中要实现三个函数，_init_,_len_,_getitem_
init方法：要传入要读取的数据集所在的目录，至少让dataset知道数据是保存在哪里的，传入磁盘路径或者传入路径
len方法：返回整个数据库大小
getitem方法：基于一个索引index返回一个训练样本
(transform函数进行归一化等一些预处理)

Dataset.py中：
实现了class dataset抽象类，类中有个方法getitem需要去实现，getitem方法是基于一个索引返回一个训练样本，由xy构成的训练队，从0开始

customimagedataset类的功能本质上讲，就是从磁盘中读取训练数据，在getitem方法中基于索引能够返回相对应的样本 
其中对于对于自己的一些任务，能够修改的比如init函数中的文件路径，定义一个自己的transform和target_transform，分别是对特征和标签的预处理

当把dataset放入dataloader之后，可以对其进行遍历，每一次遍历都会得到一个batch的特征和标签。如果设置的shuffle=true，每遍历完所有的batch之后，这个数据就会重新shuffle一遍，再遍历得到的batch和之前的就会不太一样
通过iter函数将train_dataloader函数变成迭代器，通过next函数依次从迭代器中生成一个一个批次 next(iter(train_dataloader))

dataset types有map-style datasets和iterable-style datasets
上述的dataset从磁盘读取数据集的属于前者。

Dataloader.py中：
在init函数中，第一个参数dataset通过dataset这个类实例化一个dataset对象放入dataloader中；第二个参数batch_size会设置比较大一点；第三个参数shuffle一般是必选的，shuffle=true；第四个参数sampler是自定义的一种方法从dataset中取样本（一种方法是通过shuffle，另一种就是自己写一个sampler），比如需要有序的样本；第五个参数batch_sampler也是一种自定义的一种方法，比sampler更高一级别；第六个参数num_workers是多进程的，为0的话是一个进程；第七个参数collate_fn是一个聚集函数，一般用来对一个batch后处理的；等等。
如果定义了sampler就不需要shuffle了，两者是互斥的。若定义了batch_sampler就不需要shuffle，sampler和drop_last了。（drop_last是指丢弃最后一批元素，长度小于batch_size）
